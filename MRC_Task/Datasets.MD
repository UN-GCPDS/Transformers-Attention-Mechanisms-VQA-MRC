Most used datasets for machine reading comprehension.

| Dataset | Description |  # Images | # Question-Answer pairs | Link |
|---------|-------------|-----------|-------------------------| -----|
|VisualMRC| Document images collected from the internet | 10,197 | 30,562 | https://arxiv.org/pdf/2101.11272.pdf
|SQuAD2.0 | Questions based on Wikipedia articles. The answer of every question is a segment of text from the context. It also contain unanswerable questions. | - | 150,000 | https://rajpurkar.github.io/SQuAD-explorer/
|VQA | Open ended questions about images | 256,016 | >795,000 | https://visualqa.org/
|FigureQA |Visual reasoning task, including graphical plots and figures| 100,000 | 1.3 M | https://www.microsoft.com/en-us/research/project/figureqa-dataset/highlights/ |
|ST-VQA   | Scene Text Visual Question Answering | 23,000 | 30,000 | https://paperswithcode.com/dataset/st-vqa |
|OCR-VQA | Questions about title, authot, edition, year and genre of books | 207,572 | 1 M | https://ocr-vqa.github.io/ |
|TextVQA | Designed to benchmark visual reasoning based on text in images | 28,408 | 45,336 | https://textvqa.org/ |
| DVQA | Composed of bar charts | 300,000 | 3.5 M | https://paperswithcode.com/dataset/dvqa |
|WebSRC| Web-based structural reading comprehension. It contains not only screenshots from the page, but also HTML source code. | 6,500 web pages | 440,000 | https://x-lance.github.io/WebSRC/ |
| TQA | Created for the task of Multi-Modal Machine Comprehension. The context needed to answer questions is provided and composed both of text and images. | 1,076 lessons | 26,260 | https://allenai.org/data/tqa
| Visual Genome | Multi choice setting. More balanced question type. | 101,174 | 1.7 M | https://paperswithcode.com/dataset/visual-genome |




