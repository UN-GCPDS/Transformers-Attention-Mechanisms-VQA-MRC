{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNEUxHWKXtECqINIDdrLec8"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## **Multi-head Attention**"],"metadata":{"id":"-ScQ4ZZ5hwEq"}},{"cell_type":"markdown","source":["In practice, given the same set of queries, keys, and values we may want our model to combine knowledge from different behaviors of the same attention mechanism, such as capturing dependencies of various ranges (e.g., shorter-range vs. longer-range) within a sequence. Thus,  it may be beneficial   to allow our attention mechanism to jointly use different representation subspaces of queries, keys, and values.\n","\n","\n","\n","To this end, instead of performing\n","a single attention pooling,\n","queries, keys, and values\n","can be transformed\n","with $h$ independently learned linear projections.\n","Then these $h$ projected queries, keys, and values\n","are fed into attention pooling in parallel.\n","In the end,\n","$h$ attention pooling outputs\n","are concatenated and\n","transformed with another learned linear projection\n","to produce the final output.\n","This design\n","is called *multi-head attention*,\n","where each of the $h$ attention pooling outputs\n","is a *head* `Vaswani.Shazeer.Parmar.ea.2017`."],"metadata":{"id":"dhDHduwMiD7-"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"0_LUr3h7hlpZ","executionInfo":{"status":"ok","timestamp":1688099045373,"user_tz":300,"elapsed":5583,"user":{"displayName":"Rafael Mejia Zuluaga","userId":"10854142796765961935"}}},"outputs":[],"source":["import math\n","import torch\n","from torch import nn\n","import matplotlib.pyplot as plt"]},{"cell_type":"markdown","source":["## Model\n","\n","Given a query $\\mathbf{q} \\in \\mathbb{R}^{d_q}$,\n","a key $\\mathbf{k} \\in \\mathbb{R}^{d_k}$,\n","and a value $\\mathbf{v} \\in \\mathbb{R}^{d_v}$,\n","each attention head $\\mathbf{h}_i$  ($i = 1, \\ldots, h$)\n","is computed as\n","\n","$$\\mathbf{h}_i = f(\\mathbf W_i^{(q)}\\mathbf q, \\mathbf W_i^{(k)}\\mathbf k,\\mathbf W_i^{(v)}\\mathbf v) \\in \\mathbb R^{p_v},$$\n","\n","where learnable parameters\n","$\\mathbf W_i^{(q)}\\in\\mathbb R^{p_q\\times d_q}$,\n","$\\mathbf W_i^{(k)}\\in\\mathbb R^{p_k\\times d_k}$\n","and $\\mathbf W_i^{(v)}\\in\\mathbb R^{p_v\\times d_v}$,\n","and\n","$f$ is attention pooling,\n","such as\n","additive attention and scaled dot-product attention.\n","The multi-head attention output\n","is another linear transformation via\n","learnable parameters\n","$\\mathbf W_o\\in\\mathbb R^{p_o\\times h p_v}$\n","of the concatenation of $h$ heads:\n","\n","$$\\mathbf W_o \\begin{bmatrix}\\mathbf h_1\\\\\\vdots\\\\\\mathbf h_h\\end{bmatrix} \\in \\mathbb{R}^{p_o}.$$\n","\n","Based on this design, each head may attend\n","to different parts of the input.\n","More sophisticated functions\n","than the simple weighted average can be expressed.\n","\n","## Implementation\n","\n","In our implementation,\n","we choose the scaled dot-product attention\n","for each head of the multi-head attention.\n","To avoid significant growth of computational cost and parameterization cost,\n","we set $p_q = p_k = p_v = p_o / h$.\n","Note that $h$ heads can be computed in parallel\n","if we set the number of outputs\n","of linear transformations\n","for the query, key, and value\n","to $p_q h = p_k h = p_v h = p_o$.\n","In the following implementation,\n","$p_o$ is specified via the argument `num_hiddens`."],"metadata":{"id":"6qVcR9z1i7Mb"}},{"cell_type":"code","source":["# We need the masked_softmax function and the DotProductAttention, defined in the previous notebook\n","\n","def masked_softmax(X, valid_lens):\n","    \"\"\"Perform softmax operation by masking elements on the last axis.\"\"\"\n","    # X: 3D tensor, valid_lens: 1D or 2D tensor\n","\n","    def _sequence_mask(X, valid_len, value=0):\n","        # Create a mask to hide elements beyond the valid length of each sequence\n","        maxlen = X.size(1)  # Maximum sequence length\n","        mask = torch.arange((maxlen), dtype=torch.float32,\n","                            device=X.device)[None, :] < valid_len[:, None]\n","        # The mask is a boolean tensor where each element is True if it is within\n","        # the valid length and False otherwise.\n","\n","        X[~mask] = value\n","        # Set elements outside the valid length to the given value\n","        return X\n","\n","    if valid_lens is None:\n","        # If valid_lens is not provided, perform regular softmax along the last axis\n","        return nn.functional.softmax(X, dim=-1)\n","    else:\n","        shape = X.shape\n","        if valid_lens.dim() == 1:\n","            # If valid_lens is 1D, repeat it to match the length of each sequence in X\n","            valid_lens = torch.repeat_interleave(valid_lens, shape[1])\n","        else:\n","            # If valid_lens is 2D, reshape it to 1D for further processing\n","            valid_lens = valid_lens.reshape(-1)\n","\n","        # On the last axis, replace masked elements with a very large negative\n","        # value, whose exponentiation outputs 0\n","        X = _sequence_mask(X.reshape(-1, shape[-1]), valid_lens, value=-1e6)\n","        # Reshape X to a 2D tensor to apply the sequence mask\n","        # Replace masked elements with a very large negative value (-1e6 in this case)\n","\n","        return nn.functional.softmax(X.reshape(shape), dim=-1)\n","        # Reshape X back to its original shape and apply softmax along the last axis\n","\n","class DotProductAttention(nn.Module):\n","    \"\"\"Scaled dot product attention.\"\"\"\n","    def __init__(self, dropout):\n","        super().__init__()\n","        self.dropout = nn.Dropout(dropout)\n","\n","    # Shape of queries: (batch_size, no. of queries, d)\n","    # Shape of keys: (batch_size, no. of key-value pairs, d)\n","    # Shape of values: (batch_size, no. of key-value pairs, value dimension)\n","    # Shape of valid_lens: (batch_size,) or (batch_size, no. of queries)\n","    def forward(self, queries, keys, values, valid_lens=None):\n","        d = queries.shape[-1]\n","        # Swap the last two dimensions of keys with keys.transpose(1, 2)\n","        scores = torch.bmm(queries, keys.transpose(1, 2)) / math.sqrt(d)\n","        self.attention_weights = masked_softmax(scores, valid_lens)\n","        return torch.bmm(self.dropout(self.attention_weights), values)\n","\n","class MultiHeadAttention(DotProductAttention):\n","    \"\"\"Multi-head attention.\"\"\"\n","    def __init__(self, num_hiddens, num_heads, dropout, bias=False, **kwargs):\n","        super().__init__(dropout=dropout)\n","        self.num_heads = num_heads\n","        self.attention = DotProductAttention(dropout)\n","        self.W_q = nn.LazyLinear(num_hiddens, bias=bias)\n","        self.W_k = nn.LazyLinear(num_hiddens, bias=bias)\n","        self.W_v = nn.LazyLinear(num_hiddens, bias=bias)\n","        self.W_o = nn.LazyLinear(num_hiddens, bias=bias)\n","\n","    def forward(self, queries, keys, values, valid_lens):\n","        # Shape of queries, keys, or values:\n","        # (batch_size, no. of queries or key-value pairs, num_hiddens)\n","        # Shape of valid_lens: (batch_size,) or (batch_size, no. of queries)\n","        # After transposing, shape of output queries, keys, or values:\n","        # (batch_size * num_heads, no. of queries or key-value pairs,\n","        # num_hiddens / num_heads)\n","        queries = self.transpose_qkv(self.W_q(queries))\n","        keys = self.transpose_qkv(self.W_k(keys))\n","        values = self.transpose_qkv(self.W_v(values))\n","\n","        if valid_lens is not None:\n","            # On axis 0, copy the first item (scalar or vector) for num_heads\n","            # times, then copy the next item, and so on\n","            valid_lens = torch.repeat_interleave(\n","                valid_lens, repeats=self.num_heads, dim=0)\n","\n","        # Shape of output: (batch_size * num_heads, no. of queries,\n","        # num_hiddens / num_heads)\n","        output = self.attention(queries, keys, values, valid_lens)\n","        # Shape of output_concat: (batch_size, no. of queries, num_hiddens)\n","        output_concat = self.transpose_output(output)\n","        return self.W_o(output_concat)\n","\n","    def transpose_qkv(self, X):\n","        \"\"\"Transposition for parallel computation of multiple attention heads.\"\"\"\n","        # Shape of input X: (batch_size, no. of queries or key-value pairs,\n","        # num_hiddens). Shape of output X: (batch_size, no. of queries or\n","        # key-value pairs, num_heads, num_hiddens / num_heads)\n","        X = X.reshape(X.shape[0], X.shape[1], self.num_heads, -1)\n","        # Shape of output X: (batch_size, num_heads, no. of queries or key-value\n","        # pairs, num_hiddens / num_heads)\n","        X = X.permute(0, 2, 1, 3)\n","        # Shape of output: (batch_size * num_heads, no. of queries or key-value\n","        # pairs, num_hiddens / num_heads)\n","        return X.reshape(-1, X.shape[2], X.shape[3])\n","\n","    def transpose_output(self, X):\n","        \"\"\"Reverse the operation of transpose_qkv.\"\"\"\n","        X = X.reshape(-1, self.num_heads, X.shape[1], X.shape[2])\n","        X = X.permute(0, 2, 1, 3)\n","        return X.reshape(X.shape[0], X.shape[1], -1)"],"metadata":{"id":"KEFyDXs_iw75","executionInfo":{"status":"ok","timestamp":1688102086562,"user_tz":300,"elapsed":4,"user":{"displayName":"Rafael Mejia Zuluaga","userId":"10854142796765961935"}}},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":["Let's test our implemented `MultiHeadAttention` class\n","using a toy example where keys and values are the same.\n","As a result,\n","the shape of the multi-head attention output\n","is (`batch_size`, `num_queries`, `num_hiddens`)."],"metadata":{"id":"nrnPnv6tonas"}},{"cell_type":"code","source":["num_hiddens, num_heads = 100, 5\n","attention = MultiHeadAttention(num_hiddens, num_heads, 0.5)\n","batch_size, num_queries, num_kvpairs = 2, 4, 6\n","valid_lens = torch.tensor([3, 2])\n","X = torch.ones((batch_size, num_queries, num_hiddens))\n","Y = torch.ones((batch_size, num_kvpairs, num_hiddens))\n","attention(X, Y, Y, valid_lens).shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"J3kS8W5-n8V8","executionInfo":{"status":"ok","timestamp":1688100761808,"user_tz":300,"elapsed":3,"user":{"displayName":"Rafael Mejia Zuluaga","userId":"10854142796765961935"}},"outputId":"ffbcd0d2-3321-41c1-9913-9e3b49ccc7f1"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n","  warnings.warn('Lazy modules are a new feature under heavy development '\n"]},{"output_type":"execute_result","data":{"text/plain":["torch.Size([2, 4, 100])"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":[],"metadata":{"id":"A7vDPM2VpPlp"},"execution_count":null,"outputs":[]}]}